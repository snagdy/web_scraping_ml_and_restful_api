{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from collections import namedtuple\n",
    "from urllib2 import urlopen, Request, quote, HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from os.path import abspath, exists\n",
    "from os import remove\n",
    "from random import randrange\n",
    "from time import sleep\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_url_list(generic_url_string, start_page, end_page):\n",
    "    \"\"\"This function simply returns a list of a generic url strings, with a page URL argument, from a starting page number to\n",
    "    an ending page number\"\"\"\n",
    "    return [generic_url_string.format(i) for i in xrange(start_page, end_page)]\n",
    "\n",
    "def write_html_to_disk(url, generic_filepath, pageno):\n",
    "    \"\"\"This function takes a URL, and a generic file path, and page number, and writes the returned HTML to disk for multi-page\n",
    "    websites. It is intended to be used as a way to process a list of URLs with page numbers.\"\"\"\n",
    "    delay = randrange(1,3)\n",
    "    sleep(delay)\n",
    "    f = urlopen(url)\n",
    "    data = f.read()\n",
    "    filename = generic_filepath.format(pageno)\n",
    "    with open(filename, \"w+\") as html_file:\n",
    "        html_file.write(data)\n",
    "        print \"Downloaded {} to {}\".format(url, filename)\n",
    "\n",
    "def write_multiple_html_to_disk_from_list(start, end, generic_filepath, url_list):\n",
    "    \"\"\"This function writes HTML objects to disk, to save on heap size\"\"\"\n",
    "    url_sublist = url_list[start:end]\n",
    "    [write_html_to_disk(url, generic_filepath, pgnum) for url, pgnum in zip(url_sublist, xrange(start+1,end+1))]\n",
    "\n",
    "def read_html_from_disk(generic_filepath, pagenum):\n",
    "    \"\"\"This function reads a saved HTML file from disk\"\"\"\n",
    "    html_filepath = abspath(generic_filepath.format(i))\n",
    "    html_from_file = urlopen(\"file:///{}\".format(html_filepath)).read()\n",
    "    remove(html_filepath)\n",
    "    print \"Read and deleted {}\".format(html_filepath)\n",
    "    return BeautifulSoup(html_from_file, 'lxml')\n",
    "\n",
    "# Below are experimental functions for serialising the HTML responses as single lines in a single file written as binary.\n",
    "# def append_html_as_line_to_file(url, filepath, delay=1):\n",
    "#     \"\"\"This function takes a URL and saves down its HTML response as a single line, to a file on disk.\n",
    "#     This is an alternative way to serialise the HTML data.\"\"\"\n",
    "#     sleep(delay)\n",
    "#     serialised_html = repr(BeautifulSoup(urlopen(url), 'lxml'))\n",
    "#     with open(filepath, \"ab+\") as f:\n",
    "#         f.write(serialised_html + \"\\n\")\n",
    "\n",
    "# def read_html_lines_from_file(filepath):\n",
    "#     \"\"\"This function takes a filepath, and reads html lines one by one and returns HTML objects.\"\"\"\n",
    "#     with open(filepath, \"rb\") as f:\n",
    "#         print [urlopen(\"file:///{}\".format(line.strip())) for line in f]\n",
    "#         return [BeautifulSoup(urlopen(\"http:///{}\".format(line)).read(), 'lxml') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE -- Not yet implemented to correctly parse the single-line HTML from a file.\n",
    "# url_list = get_url_list(\"https://nethouseprices.com/house-prices/london?page={}\", 1, 12484)\n",
    "# filepath = \"multiple_raw_htmls_as_lines.txt\"\n",
    "# [append_html_as_line_to_file(url, filepath) for url in url_list[:39]]\n",
    "# soup_kitchen = read_html_lines_from_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded https://nethouseprices.com/house-prices/london?page=1 to page1_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=2 to page2_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=3 to page3_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=4 to page4_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=5 to page5_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=6 to page6_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=7 to page7_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=8 to page8_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=9 to page9_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=10 to page10_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=11 to page11_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=12 to page12_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=13 to page13_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=14 to page14_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=15 to page15_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=16 to page16_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=17 to page17_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=18 to page18_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=19 to page19_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=20 to page20_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=21 to page21_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=22 to page22_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=23 to page23_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=24 to page24_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=25 to page25_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=26 to page26_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=27 to page27_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=28 to page28_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=29 to page29_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=30 to page30_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=31 to page31_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=32 to page32_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=33 to page33_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=34 to page34_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=35 to page35_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=36 to page36_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=37 to page37_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=38 to page38_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=39 to page39_raw.html\n",
      "Downloaded https://nethouseprices.com/house-prices/london?page=40 to page40_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page1_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page2_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page3_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page4_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page5_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page6_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page7_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page8_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page9_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page10_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page11_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page12_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page13_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page14_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page15_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page16_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page17_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page18_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page19_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page20_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page21_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page22_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page23_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page24_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page25_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page26_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page27_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page28_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page29_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page30_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page31_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page32_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page33_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page34_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page35_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page36_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page37_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page38_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page39_raw.html\n",
      "Read and deleted D:\\Git Repos\\web_scraping_ml_and_restful_api\\scratchpads\\page40_raw.html\n"
     ]
    }
   ],
   "source": [
    "# We first get our list of urls, and then write the HTML response objects from urllib2 to disk.\n",
    "# We then read our HTML response objects and return a list of BeautifulSoup objects.\n",
    "\n",
    "url_list = get_url_list(\"https://nethouseprices.com/house-prices/london?page={}\", 1, 12484)\n",
    "write_multiple_html_to_disk_from_list(0, 40, \"page{}_raw.html\", url_list)\n",
    "soup_kitchen = [read_html_from_disk(\"page{}_raw.html\", i) for i in xrange(1,41)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_page_scrape(soup_list, tag, tag_class):\n",
    "    \"\"\"This function is for getting tags from a collection of multiple html files as BeautifulSoup objects\"\"\"\n",
    "    return list(chain.from_iterable([soup.find_all(tag, class_=tag_class) for soup in soup_list]))\n",
    "\n",
    "def parse_date(date_string):\n",
    "    \"\"\"This function transforms date strings into something we can use as a datetime object\"\"\"\n",
    "    ds = date_string\n",
    "    dsl = ds.split(\" \")\n",
    "    return \" \".join([\"\".join([char if not char.isalpha() else \"\" for char in dsl[0]]), \" \".join(dsl[1:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identified from element inspections via the Chrome developer console.\n",
    "# We will then do a multiple scrape across many soup objects, and return a flattened list of each observation for each data\n",
    "# series.\n",
    "\n",
    "addresses = multi_page_scrape(soup_kitchen, \"strong\", \"street-details-head-row\")\n",
    "prices = multi_page_scrape(soup_kitchen, \"strong\", \"street-details-price-row\")\n",
    "details = multi_page_scrape(soup_kitchen, \"div\", \"street-details-row\")\n",
    "sale_dates_rows = multi_page_scrape(soup_kitchen, \"tr\", \"sold_price_row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now processes our sales date strings, to a format compatible for conversion to Python datetime objects.\n",
    "\n",
    "sale_date_strings = [i.findChildren('td')[-1].text for i in sale_dates_rows]\n",
    "cleaned_sale_date_strings = [parse_date(i) for i in sale_date_strings]\n",
    "sale_dates = [datetime.strptime(i, \"%d %B %Y\") for i in cleaned_sale_date_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our scraped data series are set below.\n",
    "\n",
    "addr = [i.find(\"a\").string.replace(u\"\\xa0\", \" \") for i in addresses]\n",
    "pxs = [float(i.string.replace(u\"\\xa3\", \"\").replace(u\",\", \"\")) for i in prices]\n",
    "property_characteristics = [[i.strip() for i in categories.string.split(\",\")] for categories in details]\n",
    "\n",
    "#  Some of our datapoints do not have flat type, so we must adapt for this.\n",
    "\n",
    "flat_type = [i[0] if len(i) == 3 else np.nan for i in property_characteristics]\n",
    "lease_type = [i[1] if len(i) == 3 else i[0] for i in property_characteristics]\n",
    "build_status = [i[2] if len(i) == 3 else i[1] for i in property_characteristics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our OpenStreetMap API JSON data getter and transformer functions. This is a big performance bottleneck.\n",
    "\n",
    "def get_geodata_object(openstreetmap_api_url):\n",
    "    \"\"\"this function takes our OpenStreetMap API URL and returns a JSON response.\n",
    "    We also write the response to disk for safe keeping\"\"\"\n",
    "    try:\n",
    "        url = openstreetmap_api_url\n",
    "        sleep(1)\n",
    "        req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        json_response_text = BeautifulSoup(urlopen(req), \"lxml\").text\n",
    "        json_response = json.loads(json_response_text)\n",
    "        with open(\"osm_json_responses.txt\", \"a+\") as outfile:  \n",
    "            json.dump(json_response, outfile)\n",
    "            outfile.write('\\n')\n",
    "        return json_response\n",
    "    except HTTPError as err:\n",
    "        if err.code == 429:\n",
    "            print \"HTTP Error 429: You've been blocked for being naughty.\"\n",
    "            json_response = json.loads(\"[]\")\n",
    "            with open('osm_json_responses.txt', 'a+') as outfile:  \n",
    "                json.dump(json_response, outfile)\n",
    "                outfile.write('\\n')\n",
    "            return []\n",
    "        else:\n",
    "            print \"HTTP Error {}: Look it up.\".format(err.code)\n",
    "            json_response = json.loads(\"[]\")\n",
    "            with open(\"osm_json_responses.txt\", \"a+\") as outfile:  \n",
    "                json.dump(json_response, outfile)\n",
    "                outfile.write('\\n')\n",
    "            return []\n",
    "\n",
    "def convert_json_to_named_tuple(json_):\n",
    "    \"\"\"This function is solely for our convenience when referencing JSON response attributes in dataset creation\"\"\"\n",
    "    return json.loads(json_, object_hook=lambda dict_: namedtuple('X', dict_.keys())(*dict_.values()))\n",
    "\n",
    "def clean_json(json_string):\n",
    "    \"\"\"This function cleans some of our JSON  keys, which clash with Python keywords\"\"\"\n",
    "    result = json.dumps(json_string)\n",
    "    result = result.replace(\"class\", \"category\")\n",
    "    result = result.replace(\"type\", \"subcategory\")\n",
    "    result = result.replace(\"osm_subcategory\", \"osm_type\")\n",
    "    return result\n",
    "\n",
    "def load_geodata_attributes(geodata_obj):\n",
    "    \"\"\"This function takes a JSON response namedtuple object and returns OpenStreetMap API attributes in a tuple\"\"\"\n",
    "    try:\n",
    "        return (geodata_obj.category,\n",
    "                geodata_obj.subcategory,\n",
    "                float(geodata_obj.importance),\n",
    "                float(geodata_obj.lon),\n",
    "                float(geodata_obj.lat))\n",
    "    except AttributeError: # this handles the case where our JSON loader did not find a JSON response from the API URL.\n",
    "        return (np.nan, np.nan, np.nan, np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our OpenStreetMap API call for each address converted to a URL.\n",
    "# This step can result in a temporary blacklist if too many requests are made in a short period of time.\n",
    "# This is why we have built a delay into our OpenStreetMap API dataset.\n",
    "\n",
    "generic_osm_query_url = \"https://nominatim.openstreetmap.org/search?q=\\\"{}\\\"&format=json\"\n",
    "\n",
    "if not exists(\"osm_json_responses.txt\"): # This condition prevents us running this expensive code if the output file exists.\n",
    "    geodata_urls = [generic_osm_query_url.format(i.replace(\" \", \"%20\")) for i in addr]\n",
    "    json_search_results = [get_geodata_object(url) for url in geodata_urls]\n",
    "    top_search_results = [result if len(result) == 0 else result[0] for result in json_search_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now convert these top search results back to JSON to make namedtuples for ease of referencing in dataseries creation.\n",
    "# This allows us to more easily load the JSON attributes without\n",
    "\n",
    "top_search_results_as_json = [clean_json(i) for i in top_search_results]\n",
    "geodata_json = [convert_json_to_named_tuple(result) for result in top_search_results_as_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add data parsed from display_name about borough as alternative to k Mean Clustering, to data frame.\n",
    "# json_search_results = [get_geodata_object(url) for url in geodata_urls]\n",
    "geodata = [load_geodata_attributes(result_named_tuple) for result_named_tuple in geodata_json]\n",
    "\n",
    "category = [i[0] for i in geodata]\n",
    "subcategory = [i[1] for i in geodata]\n",
    "importance = [i[2] for i in geodata]\n",
    "longitude = [i[3] for i in geodata]\n",
    "latitude = [i[4] for i in geodata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct our dataset.\n",
    "variables = [addr, pxs, sale_dates, flat_type, lease_type, build_status, category, subcategory, importance, longitude, latitude]\n",
    "series_names = [\"addresses\",\n",
    "                \"prices\",\n",
    "                \"sale_dates\",\n",
    "                \"flat_type\",\n",
    "                \"lease_type\",\n",
    "                \"build_status\",\n",
    "                \"category\",\n",
    "                \"subcategory\",\n",
    "                \"importance\",\n",
    "                \"longitude\",\n",
    "                \"latitude\"]\n",
    "\n",
    "# Check all series are the same length.\n",
    "if len(pxs) == sum([len(dataseries) for dataseries in variables])/len(variables):\n",
    "    # Setup dictionary for dataframe.\n",
    "    dataset = {series_name : series for series_name, series in zip(series_names, variables)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>addresses</th>\n",
       "      <th>build_status</th>\n",
       "      <th>category</th>\n",
       "      <th>flat_type</th>\n",
       "      <th>importance</th>\n",
       "      <th>latitude</th>\n",
       "      <th>lease_type</th>\n",
       "      <th>longitude</th>\n",
       "      <th>prices</th>\n",
       "      <th>sale_dates</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flat 14 Buchanan House, 7 Troubridge Square, L...</td>\n",
       "      <td>Newbuild</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Flat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Leasehold</td>\n",
       "      <td>NaN</td>\n",
       "      <td>517275.0</td>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flat 26 Buchanan House, 7 Troubridge Square, L...</td>\n",
       "      <td>Newbuild</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Flat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Leasehold</td>\n",
       "      <td>NaN</td>\n",
       "      <td>527175.0</td>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91 Dames Road, London, E7 0DW</td>\n",
       "      <td>Non-Newbuild</td>\n",
       "      <td>highway</td>\n",
       "      <td>Terraced</td>\n",
       "      <td>0.61</td>\n",
       "      <td>51.553902</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>0.023391</td>\n",
       "      <td>420000.0</td>\n",
       "      <td>2018-09-25</td>\n",
       "      <td>secondary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82 Ramsay Road, London, E7 9EW</td>\n",
       "      <td>Non-Newbuild</td>\n",
       "      <td>highway</td>\n",
       "      <td>Terraced</td>\n",
       "      <td>0.51</td>\n",
       "      <td>51.556909</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>0.017585</td>\n",
       "      <td>420000.0</td>\n",
       "      <td>2018-09-25</td>\n",
       "      <td>residential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21 Mandela Street, London, SW9 6EL</td>\n",
       "      <td>Non-Newbuild</td>\n",
       "      <td>highway</td>\n",
       "      <td>Terraced</td>\n",
       "      <td>0.52</td>\n",
       "      <td>51.478533</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>-0.110912</td>\n",
       "      <td>560000.0</td>\n",
       "      <td>2018-09-25</td>\n",
       "      <td>residential</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           addresses  build_status category  \\\n",
       "0  Flat 14 Buchanan House, 7 Troubridge Square, L...      Newbuild      NaN   \n",
       "1  Flat 26 Buchanan House, 7 Troubridge Square, L...      Newbuild      NaN   \n",
       "2                      91 Dames Road, London, E7 0DW  Non-Newbuild  highway   \n",
       "3                     82 Ramsay Road, London, E7 9EW  Non-Newbuild  highway   \n",
       "4                 21 Mandela Street, London, SW9 6EL  Non-Newbuild  highway   \n",
       "\n",
       "  flat_type  importance   latitude lease_type  longitude    prices sale_dates  \\\n",
       "0      Flat         NaN        NaN  Leasehold        NaN  517275.0 2018-09-26   \n",
       "1      Flat         NaN        NaN  Leasehold        NaN  527175.0 2018-09-26   \n",
       "2  Terraced        0.61  51.553902   Freehold   0.023391  420000.0 2018-09-25   \n",
       "3  Terraced        0.51  51.556909   Freehold   0.017585  420000.0 2018-09-25   \n",
       "4  Terraced        0.52  51.478533   Freehold  -0.110912  560000.0 2018-09-25   \n",
       "\n",
       "   subcategory  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2    secondary  \n",
       "3  residential  \n",
       "4  residential  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will view our dataset's first 5 records to see our work so far.\n",
    "\n",
    "dataset_frame = pd.DataFrame(dataset)\n",
    "dataset_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will save down our dataset to a CSV and JSON files, for persistence, and manual data inspection if required.\n",
    "\n",
    "dataset_frame.to_csv(\"dataset_frame.csv\")\n",
    "dataset_frame.to_json(\"dataset_frame.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
