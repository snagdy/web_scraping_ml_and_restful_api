{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from collections import namedtuple\n",
    "from urllib2 import urlopen, Request, HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from os.path import abspath, exists\n",
    "from os import remove\n",
    "from random import randrange\n",
    "from time import sleep\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_url_list(generic_url_string, start_page, end_page):\n",
    "    \"\"\"This function simply returns a list of a generic url strings, with a page URL argument, from a starting page number to\n",
    "    an ending page number\"\"\"\n",
    "    return [generic_url_string.format(i) for i in xrange(start_page, end_page)]\n",
    "\n",
    "def write_html_to_disk(url, generic_filepath, pageno):\n",
    "    \"\"\"This function takes a URL, and a generic file path, and page number, and writes the returned HTML to disk for multi-page\n",
    "    websites. It is intended to be used as a way to process a list of URLs with page numbers.\"\"\"\n",
    "    delay = randrange(1,3)\n",
    "    sleep(delay)\n",
    "    f = urlopen(url)\n",
    "    data = f.read()\n",
    "    filename = generic_filepath.format(pageno)\n",
    "    with open(filename, \"w+\") as html_file:\n",
    "        html_file.write(data)\n",
    "#         print \"Downloaded {} to {}\".format(url, filename)\n",
    "\n",
    "def write_multiple_html_to_disk_from_list(start, end, generic_filepath, url_list):\n",
    "    \"\"\"This function writes HTML objects to disk, to save on heap size\"\"\"\n",
    "    url_sublist = url_list[start:end]\n",
    "    [write_html_to_disk(url, generic_filepath, pgnum) for url, pgnum in zip(url_sublist, xrange(start+1,end+1))]\n",
    "\n",
    "def read_html_from_disk(generic_filepath, pagenum):\n",
    "    \"\"\"This function reads a saved HTML file from disk\"\"\"\n",
    "    html_filepath = abspath(generic_filepath.format(i))\n",
    "    html_from_file = urlopen(\"file:///{}\".format(html_filepath)).read()\n",
    "    remove(html_filepath)\n",
    "#     print \"Read and deleted {}\".format(html_filepath)\n",
    "    return BeautifulSoup(html_from_file, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first get our list of urls, and then write the HTML response objects from urllib2 to disk.\n",
    "# We then read our HTML response objects and return a list of BeautifulSoup objects.\n",
    "\n",
    "url_list = get_url_list(\"https://nethouseprices.com/house-prices/london?page={}\", 1, 12484)\n",
    "write_multiple_html_to_disk_from_list(0, 40, \"page{}_raw.html\", url_list)\n",
    "soup_kitchen = [read_html_from_disk(\"page{}_raw.html\", i) for i in xrange(1,41)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_page_scrape(soup_list, tag, tag_class):\n",
    "    \"\"\"This function is for getting tags from a collection of multiple html files as BeautifulSoup objects\"\"\"\n",
    "    return list(chain.from_iterable([soup.find_all(tag, class_=tag_class) for soup in soup_list]))\n",
    "\n",
    "def parse_date(date_string):\n",
    "    \"\"\"This function transforms date strings into something we can use as a datetime object\"\"\"\n",
    "    ds = date_string\n",
    "    dsl = ds.split(\" \")\n",
    "    return \" \".join([\"\".join([char if not char.isalpha() else \"\" for char in dsl[0]]), \" \".join(dsl[1:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identified from element inspections via the Chrome developer console.\n",
    "# We will then do a multiple scrape across many soup objects, and return a flattened list of each observation for each data\n",
    "# series.\n",
    "\n",
    "addresses = multi_page_scrape(soup_kitchen, \"strong\", \"street-details-head-row\")\n",
    "prices = multi_page_scrape(soup_kitchen, \"strong\", \"street-details-price-row\")\n",
    "details = multi_page_scrape(soup_kitchen, \"div\", \"street-details-row\")\n",
    "sale_dates_rows = multi_page_scrape(soup_kitchen, \"tr\", \"sold_price_row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now processes our sales date strings, to a format compatible for conversion to Python datetime objects.\n",
    "\n",
    "sale_date_strings = [i.findChildren('td')[-1].text for i in sale_dates_rows]\n",
    "cleaned_sale_date_strings = [parse_date(i) for i in sale_date_strings]\n",
    "sale_dates = [datetime.strptime(i, \"%d %B %Y\") for i in cleaned_sale_date_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our scraped data series are set below.\n",
    "\n",
    "addr = [i.find(\"a\").string.replace(u\"\\xa0\", \" \") for i in addresses]\n",
    "pxs = [float(i.string.replace(u\"\\xa3\", \"\").replace(u\",\", \"\")) for i in prices]\n",
    "property_characteristics = [[i.strip() for i in categories.string.split(\",\")] for categories in details]\n",
    "\n",
    "#  Some of our datapoints do not have flat type, so we must adapt for this.\n",
    "\n",
    "flat_type = [i[0] if len(i) == 3 else np.nan for i in property_characteristics]\n",
    "lease_type = [i[1] if len(i) == 3 else i[0] for i in property_characteristics]\n",
    "build_status = [i[2] if len(i) == 3 else i[1] for i in property_characteristics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our OpenStreetMap API JSON data getter and transformer functions. This is a big performance bottleneck.\n",
    "\n",
    "def get_geodata_object(openstreetmap_api_url):\n",
    "    \"\"\"this function takes our OpenStreetMap API URL and returns a JSON response.\n",
    "    We also write the response to disk for safe keeping\"\"\"\n",
    "    try:\n",
    "        url = openstreetmap_api_url\n",
    "        sleep(1)\n",
    "        req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        json_response_text = BeautifulSoup(urlopen(req), \"lxml\").text\n",
    "        json_response = json.loads(json_response_text)\n",
    "        with open(\"osm_json_responses.txt\", \"a+\") as outfile:  \n",
    "            json.dump(json_response, outfile)\n",
    "            outfile.write('\\n')\n",
    "        return json_response\n",
    "    except HTTPError as err:\n",
    "        if err.code == 429:\n",
    "            print \"HTTP Error 429: You've been blocked for being naughty.\"\n",
    "            json_response = json.loads(\"[]\")\n",
    "            with open('osm_json_responses.txt', 'a+') as outfile:  \n",
    "                json.dump(json_response, outfile)\n",
    "                outfile.write('\\n')\n",
    "            return []\n",
    "        else:\n",
    "            print \"HTTP Error {}: Look it up.\".format(err.code)\n",
    "            json_response = json.loads(\"[]\")\n",
    "            with open(\"osm_json_responses.txt\", \"a+\") as outfile:  \n",
    "                json.dump(json_response, outfile)\n",
    "                outfile.write('\\n')\n",
    "            return []\n",
    "\n",
    "def convert_json_to_named_tuple(json_):\n",
    "    \"\"\"This function is solely for our convenience when referencing JSON response attributes in dataset creation\"\"\"\n",
    "    return json.loads(json_, object_hook=lambda dict_: namedtuple('X', dict_.keys())(*dict_.values()))\n",
    "\n",
    "def clean_json(json_string):\n",
    "    \"\"\"This function cleans some of our JSON  keys, which clash with Python keywords\"\"\"\n",
    "    result = json.dumps(json_string)\n",
    "    result = result.replace(\"class\", \"category\")\n",
    "    result = result.replace(\"type\", \"subcategory\")\n",
    "    result = result.replace(\"osm_subcategory\", \"osm_type\")\n",
    "    return result\n",
    "\n",
    "def load_geodata_attributes(geodata_obj):\n",
    "    \"\"\"This function takes a JSON response namedtuple object and returns OpenStreetMap API attributes in a tuple\"\"\"\n",
    "    try:\n",
    "        return (geodata_obj.category,\n",
    "                geodata_obj.subcategory,\n",
    "                float(geodata_obj.importance),\n",
    "                float(geodata_obj.lon),\n",
    "                float(geodata_obj.lat))\n",
    "    except AttributeError: # this handles the case where our JSON loader did not find a JSON response from the API URL.\n",
    "        return (np.nan, np.nan, np.nan, np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our OpenStreetMap API call for each address converted to a URL.\n",
    "# This step can result in a temporary blacklist if too many requests are made in a short period of time.\n",
    "# This is why we have built a delay into our OpenStreetMap API dataset.\n",
    "\n",
    "generic_osm_query_url = \"https://nominatim.openstreetmap.org/search?q=\\\"{}\\\"&format=json\"\n",
    "\n",
    "# if not exists(\"osm_json_responses.txt\"): # This condition prevents us running this expensive code if the output file exists.\n",
    "geodata_urls = [generic_osm_query_url.format(i.replace(\" \", \"%20\")) for i in addr]\n",
    "json_search_results = [get_geodata_object(url) for url in geodata_urls]\n",
    "top_search_results = [result if len(result) == 0 else result[0] for result in json_search_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now convert these top search results back to JSON to make namedtuples for ease of referencing in dataseries creation.\n",
    "# This allows us to more easily load the JSON attributes without\n",
    "\n",
    "top_search_results_as_json = [clean_json(i) for i in top_search_results]\n",
    "geodata_json = [convert_json_to_named_tuple(result) for result in top_search_results_as_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#TO-DO: add data parsed from display_name about borough as alternative to k Mean Clustering, to data frame.\n",
    "# json_search_results = [get_geodata_object(url) for url in geodata_urls]\n",
    "geodata = [load_geodata_attributes(result_named_tuple) for result_named_tuple in geodata_json]\n",
    "\n",
    "category = [i[0] for i in geodata]\n",
    "subcategory = [i[1] for i in geodata]\n",
    "importance = [i[2] for i in geodata]\n",
    "longitude = [i[3] for i in geodata]\n",
    "latitude = [i[4] for i in geodata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct our dataset.\n",
    "variables = [addr, pxs, sale_dates, flat_type, lease_type, build_status, category, subcategory, importance, longitude, latitude]\n",
    "series_names = [\"addresses\",\n",
    "                \"prices\",\n",
    "                \"sale_dates\",\n",
    "                \"flat_type\",\n",
    "                \"lease_type\",\n",
    "                \"build_status\",\n",
    "                \"category\",\n",
    "                \"subcategory\",\n",
    "                \"importance\",\n",
    "                \"longitude\",\n",
    "                \"latitude\"]\n",
    "\n",
    "# Check all series are the same length.\n",
    "if len(pxs) == sum([len(dataseries) for dataseries in variables])/len(variables):\n",
    "    # Setup dictionary for dataframe.\n",
    "    dataset = {series_name : series for series_name, series in zip(series_names, variables)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>addresses</th>\n",
       "      <th>build_status</th>\n",
       "      <th>category</th>\n",
       "      <th>flat_type</th>\n",
       "      <th>importance</th>\n",
       "      <th>latitude</th>\n",
       "      <th>lease_type</th>\n",
       "      <th>longitude</th>\n",
       "      <th>prices</th>\n",
       "      <th>sale_dates</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49 Downsfield Road, London, E17 8BY</td>\n",
       "      <td>Non-Newbuild</td>\n",
       "      <td>highway</td>\n",
       "      <td>Terraced</td>\n",
       "      <td>0.520</td>\n",
       "      <td>51.576340</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>-0.031657</td>\n",
       "      <td>560000.0</td>\n",
       "      <td>2018-10-29</td>\n",
       "      <td>residential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95 Bedford Road, London, N2 9DB</td>\n",
       "      <td>Non-Newbuild</td>\n",
       "      <td>place</td>\n",
       "      <td>Terraced</td>\n",
       "      <td>0.421</td>\n",
       "      <td>51.461539</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>-0.129039</td>\n",
       "      <td>1550000.0</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18 Oakfield Road, London, N4 4NL</td>\n",
       "      <td>Non-Newbuild</td>\n",
       "      <td>place</td>\n",
       "      <td>Terraced</td>\n",
       "      <td>0.421</td>\n",
       "      <td>51.595599</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>-0.033173</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22 Ridge Road, London, N21 3EA</td>\n",
       "      <td>Non-Newbuild</td>\n",
       "      <td>place</td>\n",
       "      <td>Terraced</td>\n",
       "      <td>0.411</td>\n",
       "      <td>41.615279</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>-72.089979</td>\n",
       "      <td>535000.0</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>232 East End Road, London, N2 8AX</td>\n",
       "      <td>Non-Newbuild</td>\n",
       "      <td>highway</td>\n",
       "      <td>Semi Detached</td>\n",
       "      <td>0.610</td>\n",
       "      <td>51.593683</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>-0.183207</td>\n",
       "      <td>1100000.0</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>primary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             addresses  build_status category      flat_type  \\\n",
       "0  49 Downsfield Road, London, E17 8BY  Non-Newbuild  highway       Terraced   \n",
       "1      95 Bedford Road, London, N2 9DB  Non-Newbuild    place       Terraced   \n",
       "2     18 Oakfield Road, London, N4 4NL  Non-Newbuild    place       Terraced   \n",
       "3       22 Ridge Road, London, N21 3EA  Non-Newbuild    place       Terraced   \n",
       "4    232 East End Road, London, N2 8AX  Non-Newbuild  highway  Semi Detached   \n",
       "\n",
       "   importance   latitude lease_type  longitude     prices sale_dates  \\\n",
       "0       0.520  51.576340   Freehold  -0.031657   560000.0 2018-10-29   \n",
       "1       0.421  51.461539   Freehold  -0.129039  1550000.0 2018-10-26   \n",
       "2       0.421  51.595599   Freehold  -0.033173  1575000.0 2018-10-26   \n",
       "3       0.411  41.615279   Freehold -72.089979   535000.0 2018-10-26   \n",
       "4       0.610  51.593683   Freehold  -0.183207  1100000.0 2018-10-26   \n",
       "\n",
       "   subcategory  \n",
       "0  residential  \n",
       "1        house  \n",
       "2        house  \n",
       "3        house  \n",
       "4      primary  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will view our dataset's first 5 records to see our work so far.\n",
    "\n",
    "dataset_frame = pd.DataFrame(dataset)\n",
    "dataset_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save down our dataset to a CSV and JSON files, for persistence, and manual data inspection if required.\n",
    "\n",
    "dataset_frame.to_csv(\"dataset_frame.csv\")\n",
    "dataset_frame.to_json(\"dataset_frame.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
