{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from pprint import pprint as pprint\n",
    "from re import sub\n",
    "from collections import namedtuple\n",
    "from urllib2 import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://nethouseprices.com/house-prices/london?page=1\"\n",
    "# headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:12.0) Gecko/20100101 Firefox/12.0'}\n",
    "# req = Request(url, None, headers)\n",
    "# html = urlopen(req)\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, \"xml\")\n",
    "# TO-DO: segment HTML downloading into chunks, and see if HTML can be saved to disk, and scraped by BeautifulSoup from disk.\n",
    "# Also determine how much of the data to use as a dataset, and turn all your processing code into functions, so you can do\n",
    "# this processing across multiple URLs to get one dataset you can model on.\n",
    "# The next step is to do k-Means Clustering and get dummies, then run Random Forest Regressor, and write RESTful API.\n",
    "# return [\"https://nethouseprices.com/house-prices/london?page={}\".format(i) for i in xrange(1,12484)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we download our entire dataset HTMLs in chunks, and write them to disk, only opening them when we need them.\n",
    "# def get_url_list(generic_url_string, start_page, end_page):\n",
    "#     return [\"https://nethouseprices.com/house-prices/london?page={}\".format(i) for i in xrange(start_page, end_page)]\n",
    "\n",
    "# def write_html_to_disk(url, start, end):\n",
    "#     print start, end\n",
    "#     f = urlopen(url)\n",
    "#     data = f.read()\n",
    "#     with open(\"{}_{}_raw.html\".format(start, end), \"a+\") as html_file:\n",
    "#         html_file.write(data)\n",
    "\n",
    "# def write_multiple_html_to_disk_from_list(start, end, chunk_size, url_list):\n",
    "#     start_indices = list(xrange(start, end, chunk_size))\n",
    "#     end_index_offset = chunk_size-1\n",
    "#     [[write_html_to_disk(url, s, s + end_index_offset) for url in url_list[start-1:end-1]] for s in start_indices]\n",
    "\n",
    "    \n",
    "# url_list = get_url_list(\"https://nethouseprices.com/house-prices/london?page={}\", 1, 12484)\n",
    "# write_multiple_html_to_disk_from_list(0, 10, 10, url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Identified from element inspections via the Chrome developer console.\n",
    "addresses = soup.find_all(\"strong\", class_=\"street-details-head-row\")\n",
    "prices = soup.find_all(\"strong\", class_=\"street-details-price-row\")\n",
    "details = soup.find_all(\"div\", class_=\"street-details-row\")\n",
    "print addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sale_dates_rows = soup.find_all(\"tr\", class_=\"sold_price_row\")\n",
    "sale_date_strings = [i.findChildren('td')[-1].text for i in sale_dates_rows]\n",
    "def parse_date(date_string):\n",
    "    ds = date_string\n",
    "    dsl = ds.split(\" \")\n",
    "    return \" \".join([\"\".join([char if not char.isalpha() else \"\" for char in dsl[0]]), \" \".join(dsl[1:])])\n",
    "\n",
    "cleaned_sale_date_strings = [parse_date(i) for i in sale_date_strings]\n",
    "# print cleaned_sale_date_strings\n",
    "sale_dates = [datetime.strptime(i, \"%d %B %Y\") for i in cleaned_sale_date_strings]\n",
    "# sale_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our scraped data series are set below.\n",
    "addr = [i.find(\"a\").string.replace(u\"\\xa0\", \" \") for i in addresses]\n",
    "pxs = [float(i.string.replace(u\"\\xa3\", \"\").replace(u\",\", \"\")) for i in prices]\n",
    "property_characteristics = [[i.strip() for i in categories.string.split(\",\")] for categories in details]\n",
    "flat_type = [i[0] for i in property_characteristics]\n",
    "lease_type = [i[1] for i in property_characteristics]\n",
    "build_status = [i[2] for i in property_characteristics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geodata_urls = [\"https://nominatim.openstreetmap.org/search?q=\\\"{}\\\"&format=json\".format(i.replace(\" \", \"%20\")) for i in addr]\n",
    "    \n",
    "def get_geodata_object(openstreetmap_api_url):\n",
    "    url = openstreetmap_api_url\n",
    "    json_response_text = BeautifulSoup(urlopen(url), \"lxml\").text\n",
    "    return json.loads(json_response_text)\n",
    "\n",
    "def convert_json_to_named_tuple(json_):\n",
    "    \"\"\"This is solely for our convenience when referencing JSON response attributes in dataset creation\"\"\"\n",
    "    return json.loads(json_, object_hook=lambda dict_: namedtuple('X', dict_.keys())(*dict_.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_search_results = [get_geodata_object(url) for url in geodata_urls]\n",
    "top_search_results = [result if len(result) == 0 else result[0] for result in json_search_results]\n",
    "# for i in top_search_results:\n",
    "#     pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now convert these top search results back to JSON to make named tuples for ease of referencing in dataseries creation.\n",
    "# top_search_results_as_strings = ['[{}]'.format(str(i)) if type(i) == dict else '{}'.format(i) for i in top_search_results]\n",
    "top_search_results_as_json = [json.dumps(i).replace(\"class\", \"category\").replace(\"type\", \"subcategory\").replace(\"osm_subcategory\", \"osm_type\") for i in top_search_results]\n",
    "geodata_json = [convert_json_to_named_tuple(result) for result in top_search_results_as_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_geodata_attributes(geodata_obj):\n",
    "    try:\n",
    "        return (\n",
    "                geodata_obj.category,\n",
    "                geodata_obj.subcategory,\n",
    "                float(geodata_obj.importance),\n",
    "                float(geodata_obj.lon),\n",
    "                float(geodata_obj.lat))\n",
    "    except AttributeError: # this handles the case where our JSON loader did not find a JSON response from the API URL.\n",
    "        return (np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "#TO-DO: add data parsed from display_name about borough as alternative to k Mean Clustering, to data frame.\n",
    "# json_search_results = [get_geodata_object(url) for url in geodata_urls]\n",
    "geodata = [load_geodata_attributes(result_named_tuple) for result_named_tuple in geodata_json]\n",
    "\n",
    "category = [i[0] for i in geodata]\n",
    "subcategory = [i[1] for i in geodata]\n",
    "importance = [i[2] for i in geodata]\n",
    "longitude = [i[3] for i in geodata]\n",
    "latitude = [i[4] for i in geodata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We construct our dataset.\n",
    "variables = [addr, pxs, sale_dates, flat_type, lease_type, build_status, category, subcategory, importance, longitude, latitude]\n",
    "series_names = [\"addresses\",\n",
    "                \"prices\",\n",
    "                \"sale_dates\",\n",
    "                \"flat_type\",\n",
    "                \"lease_type\",\n",
    "                \"build_status\",\n",
    "                \"category\",\n",
    "                \"subcategory\",\n",
    "                \"importance\",\n",
    "                \"longitude\",\n",
    "                \"latitude\"]\n",
    "\n",
    "# Check all series are the same length.\n",
    "if len(pxs) == sum([len(dataseries) for dataseries in variables])/len(variables):\n",
    "    # Setup dictionary for dataframe.\n",
    "    dataset = {series_name : series for series_name, series in zip(series_names, variables)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Beware some erroneous lon-lat data due to multiple search results and improper parsing of JSON response.\n",
    "# NB: ignore the above, this was fixed. Now have to find a way of removing bad search data, say data 1SD away from mean lat lon.\n",
    "\n",
    "dataset_frame = pd.DataFrame(dataset)\n",
    "dataset_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We add a new parameter, which we will use to exclude anomalous lat lon coordinates from bad Open Street Map API data.\n",
    "    \n",
    "dataset_frame[\"latitude_z_score\"] = (dataset_frame.latitude - dataset_frame.latitude.mean()) / dataset_frame.latitude.std()\n",
    "dataset_frame[\"longitude_z_score\"] = (dataset_frame.longitude - dataset_frame.longitude.mean()) / dataset_frame.longitude.std()\n",
    "dataset_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We create new columns which introduce NaN data where the lat lon absolute Z scores exceed 3, indicating likely bad data.\n",
    "# By bad data, we mean assuming a normal distribution, the lat lon coordinates are statistically significantly different\n",
    "# from the average coordinates of all other datapoints. Given all the houses should be in London, this is a good filter.\n",
    "\n",
    "dataset_frame[\"lat_z_score_mask\"] = dataset_frame.latitude_z_score.where(dataset_frame.latitude_z_score.abs() < 3)\n",
    "dataset_frame[\"lon_z_score_mask\"] = dataset_frame.longitude_z_score.where(dataset_frame.longitude_z_score.abs() < 3)\n",
    "dataset_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will also create dummy variables for all our non-numerical dataseries.\n",
    "build_status_dummies = pd.get_dummies(dataset_frame.build_status)\n",
    "flat_type_dummies = pd.get_dummies(dataset_frame.flat_type)\n",
    "lease_type_dummies = pd.get_dummies(dataset_frame.lease_type)\n",
    "category_dummies = pd.get_dummies(dataset_frame.category)\n",
    "subcategory_dummies = pd.get_dummies(dataset_frame.subcategory)\n",
    "\n",
    "\n",
    "dataframes_set = [dataset_frame, build_status_dummies, flat_type_dummies, lease_type_dummies, category_dummies, subcategory_dummies]\n",
    "# for dataframe in dataframes_set:\n",
    "#     dataframe.reset_index(drop=True)\n",
    "    \n",
    "dataset_frame = pd.concat(dataframes_set, axis=1)\n",
    "# dataset_frame = pd.concat([dataset_frame, pd.get_dummies(dataset_frame.lease_type)], axis=1)\n",
    "dataset_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_frame = dataset_frame.dropna()\n",
    "cols_to_drop = ['build_status',\n",
    "                'flat_type',\n",
    "                'lease_type',\n",
    "                'category',\n",
    "                'subcategory',\n",
    "                'latitude_z_score',\n",
    "                'longitude_z_score',\n",
    "                'lat_z_score_mask',\n",
    "                'lon_z_score_mask']\n",
    "final_dataset_frame = dataset_frame.drop(cols_to_drop, axis=1)\n",
    "final_dataset_frame = final_dataset_frame.reset_index(drop=True)\n",
    "final_dataset_frame.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
